{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rip-One-Rule Algorithm\n",
    "\n",
    "## Overview\n",
    "The purpose of this code is to create a robust simple flexible Decision Rule algorithm. It combines elements from the two simplest Decisiton rule algorithms: OneR and Ripper. Unlike a deicsion tree, a decision rule algorithm is designed to find the best rules for sub-segments rather than finding the best overall set of rules to fit the entire population. As such it is suitable to fit a portion of the population or only a portion of the population. But it will find the best rules for that portion.\n",
    "\n",
    "## Decision Rule Algorithms \n",
    "Decision Rules have largely gotten lost in the maelstrom of Machine Learning Techinques out there. I think for many there really isn't much of a need. As such there really is only one good overview of the topic that I was able to find: \n",
    "https://christophm.github.io/interpretable-ml-book/rules.html\n",
    "Despite this I feel very strongly that there is a clear need for development in this area. The reason is implementability in the business world and the need for greater transparency in AI and ML overall. As such a decision rule algorithm can work in cases where decision trees or clustering algorithms can not. \n",
    "\n",
    "The existing algorithms lacked the flexibility to determine the \"Best\" rule that was necessary for the particular project this was developed for. It combines what I view as the best of the two most basic Decision Rule Algorithms: from OneRule it finds the best first rule and from Ripper it utilizes sequential covering. It that sense it is not particularly origional, but having a need for this particular kind of solution and not finding a good out of the box alternative I felt it was worth developing this tool in a general way. There is one more specific goal, the purpose of this algorithm is to generate the most bang for your buck. Not only do we want the best rules we want the fewest number of them, and we want to find them very quickly. This increases the number of threshold criteria, but allows for a control between speed and robustness.  \n",
    "\n",
    "## Algorithm Overview\n",
    "Speifically this algorithm utilizes a pandas dataframe rather than a Numpy matrix. There are two reasons for this. First, I want to keep the labeling attached and second I wanted to convert this to pyspark.  \n",
    "\n",
    "The first thing the model does is to create (or is supplied) a list of feature variables and the name of a target. For now these variables must be categorical in nature. We will look to catagorizaiton schema for furture versions of this model.\n",
    "\n",
    "### Sequential Covering \n",
    "0) We first start with an empty list of rules.  \n",
    "1) Learn a rule. (See below)   \n",
    "2) If you find a rule that meets criteria you add it to the list.  \n",
    "3) Then remove all data points covered by rule.   \n",
    "4) While the list of rules is below a certain quantity threshold or the last rule is above a quality threshold, you would find another rule. Else (go to 7).  \n",
    "5) Learn another rule on the remaining data (return to 1).  \n",
    "6) Return the decision list.  \n",
    "\n",
    "### Rule Finding\n",
    "1) It takes the first feature and does a group by on the table of that feature variable and the target and returns a valuation of that in a third column. This valuation can be chosen from any of the available aggregators in pandas agg (e.g. avg, sum, median etc.).   \n",
    "2) Check if that rule meets basic thresholds (size and accuracy). If so keep, and decend to level 2 of  pruned tree search.  \n",
    "3) Find a rule using the first feature and one of the other variables. If it meets the two basic thresholds and is greater than the top rule keep and decend again.   \n",
    "4) If its not better or if it doesn't meet decent thresholds move to the next variable combination (Feature 1 and Feature 3).   \n",
    "5) When all variable combinations have been exhausted for a given level move back up to the previous level.   \n",
    "6) When you are back on level 1 continue on to the next feature and repeat 1-5, untill you have completed the entire list. Each time replacing the current best rule with another best rule and so on.   \n",
    "\n",
    "Note on 1) There is even the possibility of custom evaluators. Be careful with these as they could radically slow your code.   \n",
    "Note on 2) Unlike One rule not all categories are kept. As a result, this model will not cover the entire space with one rule.   \n",
    "Other note on 2) If you want to find the true OneR rule, then set the size and accuracy thresholds to zero. This will not span the whole data set in one rule but will find the rule maximizing that potential, doing so however sets the speed of this algorithm to just over n^2\n",
    "\n",
    "This will not create a decision set, but rather a decision list (order matters).\n",
    "It can be done against all different values of the target but it is not necessarily recommended to do so. \n",
    "\n",
    "This whole process should be just under order n^2.\n",
    "\n",
    "## Theoretical Background\n",
    "The physical description of this process is like throwing coins into bags. The bags do not have to be the same size and the coins do not have to be two sided. From the binomial coefficient theorem we know that for this random system the possible range of combinations (number of coins, percent heads) are strongly dependent on each other. The smaller the sample size (number of coints) the much greater the range of coins coming up heads (percentage) is possible. These systems are likely too complicated to calcualte analytically. That is why an error checker is available to brute force the calculation. It will randomize your feature or target variable. and then rerun the calcualtion with the same thresholds (except that it will discard target labels and keep everything.) It then counts all the possible random combinations for the first rule. To make this calculation accurate you need to set much lower thresholds and also a depth limit (so it actually stops). What this will show for any case is that for small numbers of coinds in a bin there is no percentage which is statistically significant and for large bags the signficance is shockingly low. \n",
    "\n",
    "\n",
    "## Current Implementation\n",
    "\n",
    "While there are other possible aggreagators, only two are implemented here. A straight up classifier that focuses on maximizing the accuracy of the rules that are selected and a gready aggregator that trys to maximize the sum of some value for each rule. The latter is best for situations where the type of object of interest connects to some positve value, and the other categories have some negative value. For example we could look at completed sales vs returned sales, where completed sales have some profit and the returns have a loss. In that case we could look to the greedy aggregator to maximise the target value with the minimum number of rules. In either case the size threshold is still important, though in the latter case particularly for early rules they will be maximally significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headers and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cap_shape</th>\n",
       "      <th>cap_surface</th>\n",
       "      <th>cap_color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill_attachment</th>\n",
       "      <th>gill_spacing</th>\n",
       "      <th>gill_size</th>\n",
       "      <th>gill_color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk_surface_below_ring</th>\n",
       "      <th>stalk_color_above_ring</th>\n",
       "      <th>stalk_color_below_ring</th>\n",
       "      <th>veil_type</th>\n",
       "      <th>veil_color</th>\n",
       "      <th>ring_number</th>\n",
       "      <th>ring_type</th>\n",
       "      <th>spore_print_color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  type cap_shape cap_surface cap_color bruises odor gill_attachment  \\\n",
       "0    p         x           s         n       t    p               f   \n",
       "1    e         x           s         y       t    a               f   \n",
       "2    e         b           s         w       t    l               f   \n",
       "3    p         x           y         w       t    p               f   \n",
       "4    e         x           s         g       f    n               f   \n",
       "\n",
       "  gill_spacing gill_size gill_color  ... stalk_surface_below_ring  \\\n",
       "0            c         n          k  ...                        s   \n",
       "1            c         b          k  ...                        s   \n",
       "2            c         b          n  ...                        s   \n",
       "3            c         n          n  ...                        s   \n",
       "4            w         b          k  ...                        s   \n",
       "\n",
       "  stalk_color_above_ring stalk_color_below_ring veil_type veil_color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring_number ring_type spore_print_color population habitat  \n",
       "0           o         p                 k          s       u  \n",
       "1           o         p                 n          n       g  \n",
       "2           o         p                 n          n       m  \n",
       "3           o         p                 k          s       u  \n",
       "4           o         e                 n          a       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "type                        object\n",
       "cap_shape                   object\n",
       "cap_surface                 object\n",
       "cap_color                   object\n",
       "bruises                     object\n",
       "odor                        object\n",
       "gill_attachment             object\n",
       "gill_spacing                object\n",
       "gill_size                   object\n",
       "gill_color                  object\n",
       "stalk_shape                 object\n",
       "stalk_root                  object\n",
       "stalk_surface_above_ring    object\n",
       "stalk_surface_below_ring    object\n",
       "stalk_color_above_ring      object\n",
       "stalk_color_below_ring      object\n",
       "veil_type                   object\n",
       "veil_color                  object\n",
       "ring_number                 object\n",
       "ring_type                   object\n",
       "spore_print_color           object\n",
       "population                  object\n",
       "habitat                     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data_df = pd.read_csv(\"./iris_data.csv\")\n",
    "data_df = pd.read_csv('./mushrooms.csv')\n",
    "\n",
    "display(data_df.shape)\n",
    "display(data_df.head(5))\n",
    "display(data_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Object Itself\n",
    "The parameters you can pick are as follows: \n",
    "\n",
    "Things not yet done:  \n",
    "Clean up plots\n",
    "remove duplicate index\n",
    "Only aggregators are sum and count (default). Count corresponds to average accuracy. Sum is a agglomerative algorithm.\n",
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RipOneRule(object):\n",
    " \n",
    "    def __init__(self, size_threshold = 1, agg_threshold = 0, feature_list = [], target = 'Target', \n",
    "                 target_value = 'Any', aggregator ='count', max_depth = -1, max_rules = -1, \n",
    "                 max_pop = sys.maxsize, rules_list = [],rules_value = [], rules_size = [], rules_target = [],\n",
    "                 decent_flag = 'Both', stop_flag = 'Count', restart=False):\n",
    "        if restart == True:\n",
    "            self.size_threshold = size_threshold \n",
    "            self.agg_threshold = agg_threshold \n",
    "            self.feature_list = feature_list\n",
    "            self.target = target \n",
    "            self.target_value = target_value \n",
    "            self.aggregator = aggregator  \n",
    "            self.max_depth = max_depth \n",
    "            self.max_rules = max_rules \n",
    "            self.max_pop = max_pop   \n",
    "            self.prior_work_flag=True\n",
    "        else:\n",
    "            self.size_threshold = size_threshold \n",
    "            self.agg_threshold = agg_threshold \n",
    "            self.feature_list = feature_list\n",
    "            self.target = target \n",
    "            self.target_value = target_value \n",
    "            self.aggregator = aggregator  \n",
    "            self.max_depth = max_depth \n",
    "            self.max_rules = max_rules \n",
    "            self.max_pop = max_pop \n",
    "            self.rules_list = rules_list\n",
    "            self.rules_value = rules_value\n",
    "            self.rules_size = rules_size\n",
    "            self.rules_target = rules_target\n",
    "            self.prior_work_flag=True\n",
    " \n",
    "            if self.aggregator == 'count':\n",
    "                self.max_flag='percentage'\n",
    "            else:\n",
    "                self.max_flag=self.aggregator\n",
    "    \n",
    "    def fit(self, df): \n",
    "        total_pop =0 \n",
    "        still_good=True\n",
    "        df_work = df.copy()\n",
    "        print(\"starting members  = \", df_work.shape[0])\n",
    "            \n",
    "        while still_good:\n",
    "            Done = self.get_rule(df_work)\n",
    "            condition = []\n",
    "            for key,val in self.rules_list[-1].items():\n",
    "                if len(condition) == 0:\n",
    "                    condition = df_work[key] == val\n",
    "                else:                        \n",
    "                    condition = condition & (df_work[key] == val)\n",
    "            df_work = df_work.loc[~condition,:]\n",
    "            print(\"remaining members = \", df_work.shape[0])\n",
    "            \n",
    "            total_pop = total_pop - self.rules_value[-1]\n",
    " \n",
    "            if len(self.rules_list) == self.max_rules:\n",
    "                still_good= False\n",
    "            if (self.target_value == 'Any') and (df_work.shape[0] == 0):\n",
    "                still_good= False\n",
    "            if ((self.target_value != 'Any') and \n",
    "                       (df_work.loc[df_work[self.target]==self.target_value,:].shape[0] == 0)):\n",
    "                still_good= False\n",
    "            if total_pop >= self.max_pop:\n",
    "                still_good= False\n",
    "            if Done:\n",
    "                still_good= False\n",
    "            \n",
    "        print(self.rules_list)\n",
    "        print(self.rules_size)\n",
    "        print(self.rules_value)\n",
    "        print(self.rules_target)\n",
    "        \n",
    "        return df, df_work\n",
    "    \n",
    "        del df_work\n",
    " \n",
    "    def get_rule(self, df_work):\n",
    "        depth=1\n",
    "        #print(\"Depth = \", str(depth))\n",
    "        Done = False\n",
    "        var_list = []\n",
    "        try_rule_list = []\n",
    "        try_rule_size = []\n",
    "        try_rule_value = []\n",
    "        try_rule_target = []\n",
    "        df_work[self.aggregator] = 0\n",
    "        for feature in self.feature_list:\n",
    "            value_df = df_work[[feature,self.target,self.aggregator]].groupby([feature,self.target]\n",
    "                                                                    ).agg(self.aggregator).reset_index()\n",
    "            if self.aggregator == 'count':\n",
    "                count_df = value_df[[feature,self.aggregator]].rename({self.aggregator:'total'},axis=1\n",
    "                                                                ).groupby([feature]).agg('sum').reset_index()\n",
    "                temp_rules_df = pd.merge(value_df,count_df,how='left',on=feature) \n",
    "                temp_rules_df['percentage'] = temp_rules_df['count']/temp_rules_df['total']\n",
    "            else:\n",
    "                temp_rules_df= value_df    \n",
    "            if self.target_value != 'Any':\n",
    "                condition = temp_rules_df[target] == self.target_value\n",
    "                temp_rules_df = temp_rules_df.loc[condition,:]\n",
    "            temp_rules_df = temp_rules_df.loc[temp_rules_df['total'] > self.size_threshold,:]\n",
    "            if (temp_rules_df[self.max_flag].max() > self.agg_threshold): \n",
    "                max_row = temp_rules_df[self.max_flag].idxmax()\n",
    "                #print(\"Max_row = \",max_row)\n",
    "                try_rule_list.append({feature: temp_rules_df.loc[max_row,feature]}.copy())\n",
    "                try_rule_size.append(temp_rules_df.loc[max_row,'total'])\n",
    "                try_rule_value.append(temp_rules_df.loc[max_row,'percentage'])  \n",
    "#                 try_rule_value.append(temp_rules_df.loc[max_row,self.aggregator]]) \n",
    "                try_rule_target.append(temp_rules_df.loc[max_row,self.target])\n",
    "            del temp_rules_df\n",
    "            del value_df\n",
    "            del count_df  \n",
    "        if (len(try_rule_list) > 0):\n",
    "            if (self.max_depth != 1):\n",
    "                for ivar in range(len(try_rule_list)):\n",
    "                    [try_rule_list[ivar],try_rule_size[ivar],try_rule_value[ivar],try_rule_target[ivar]] = \\\n",
    "                                    self.go_deeper(df_work, depth, try_rule_list[ivar],try_rule_size[ivar],\n",
    "                                                   try_rule_value[ivar],try_rule_target[ivar])\n",
    "            iBest = np.asarray(try_rule_value).argmax()\n",
    "            self.rules_list.append(try_rule_list[iBest])\n",
    "            self.rules_size.append(try_rule_size[iBest])\n",
    "            self.rules_value.append(try_rule_value[iBest])\n",
    "            self.rules_target.append(try_rule_target[iBest])\n",
    "        else:\n",
    "            Done = True\n",
    " \n",
    "        del var_list\n",
    "        del try_rule_list\n",
    "        del try_rule_size\n",
    "        del try_rule_value\n",
    "        del try_rule_target\n",
    " \n",
    "        #print(\"Dne = \" ,Done)\n",
    "    \n",
    "        return Done \n",
    " \n",
    "    def go_deeper(self,df_work,depth,high_rule,high_rule_size,high_rule_value,high_rule_target):\n",
    "        depth = depth + 1    \n",
    "        #new_var_list = []\n",
    "        try_rule_list = []\n",
    "        try_rule_size = []\n",
    "        try_rule_value = []\n",
    "        try_rule_target = []\n",
    "        for feature in self.feature_list:\n",
    "            if feature in list(high_rule.keys()):\n",
    "                continue\n",
    "            value_df = df_work[list(high_rule.keys())+[feature,self.target,self.aggregator]].groupby(\n",
    "                                list(high_rule.keys()) + [feature,self.target]\n",
    "                                ).agg(self.aggregator).reset_index() \n",
    "            if self.aggregator == 'count':\n",
    "                count_df = value_df[list(high_rule.keys())+[feature,self.aggregator]].rename(\n",
    "                                    {self.aggregator:'total'},axis=1\n",
    "                                    ).groupby(list(high_rule.keys())+[feature]).agg('sum').reset_index()\n",
    "                temp_rules_df = pd.merge(value_df,count_df,how='left',on=list(high_rule.keys())+[feature]) \n",
    "                temp_rules_df['percentage'] = temp_rules_df['count']/temp_rules_df['total']\n",
    "            else:\n",
    "                temp_rules_df= value_df\n",
    "                \n",
    "            if self.target_value != 'Any':\n",
    "                condition = temp_rules_df[target] == self.target_value\n",
    "                temp_rules_df = temp_rules_df.loc[condition,:]\n",
    "            \n",
    "            temp_rules_df = temp_rules_df.loc[temp_rules_df['total'] > self.size_threshold,:]\n",
    "            #display(temp_rules_df)\n",
    "            \n",
    "            if (temp_rules_df[self.max_flag].max() > self.agg_threshold):\n",
    "                #display(temp_rules_df)\n",
    "                max_row = temp_rules_df[self.max_flag ].idxmax()\n",
    "                #print(\"max_row = \",max_row)\n",
    "                var_list = list(high_rule.keys())+[feature]\n",
    "                #print(var_list)\n",
    "                rule_dict ={}\n",
    "                for var in var_list:\n",
    "                    rule_dict.update({var:temp_rules_df.loc[max_row,var]})\n",
    "                try_rule_list.append(rule_dict)\n",
    "                try_rule_size.append(temp_rules_df.loc[max_row,'total'])\n",
    "                try_rule_value.append(temp_rules_df.loc[max_row,'percentage'])  \n",
    "#                 try_rule_value.append(temp_rules_df.loc[max_row,self.aggregator]]) \n",
    "                try_rule_target.append(temp_rules_df.loc[max_row,self.target])\n",
    "            del value_df\n",
    "            del count_df\n",
    "            \n",
    "        if (len(try_rule_list) > 0): \n",
    "            if(self.max_depth != depth):\n",
    "                for ivar in range(len(try_rule_list)):\n",
    "                    [try_rule_list[ivar],try_rule_size[ivar],try_rule_value[ivar],try_rule_target[ivar]] = \\\n",
    "                                            self.go_deeper(df_work,depth,\n",
    "                                            try_rule_list[ivar],try_rule_size[ivar],try_rule_value[ivar],\n",
    "                                                          try_rule_target[ivar]) \n",
    "            # need to replace not append\n",
    "            iBest = np.asarray(try_rule_value).argmax()\n",
    "            if (try_rule_value[iBest] > high_rule_value):\n",
    "                high_rule = try_rule_list[iBest]\n",
    "                high_rule_value = try_rule_value[iBest]\n",
    "                high_rule_size = try_rule_size[iBest]\n",
    "                high_rule_target = try_rule_target[iBest]\n",
    "\n",
    "        depth = depth-1\n",
    "        #print(\"Depth = \", str(depth))\n",
    "        \n",
    "        return high_rule, high_rule_size, high_rule_value, high_rule_target\n",
    " \n",
    "        del temp_rules_df\n",
    "        del new_var_list\n",
    "        del try_rule_list\n",
    "        del try_rule_size\n",
    "        del try_rule_value\n",
    " \n",
    "    def predict(self, df):\n",
    "        df['Rule Number'] = 0\n",
    "        total_rules = len(self.rules_list)\n",
    "        i=0\n",
    "        for rule in reversed(self.rules_list):\n",
    "            condition = []\n",
    "            for key,val in rule.items():\n",
    "                if len(condition) == 0:\n",
    "                    condition = df[key] == val\n",
    "                else:                        \n",
    "                    condition = condition & df[key] == val\n",
    "            df.loc[condition,'Rule Number'] = total_rules-i\n",
    "            i=i-1\n",
    "            \n",
    "        return df\n",
    " \n",
    "    # need to fix these next two\n",
    "    def check_significance(self,df,scramble=False,scale=10):\n",
    "        try_rule_list=[]\n",
    "        Done=False\n",
    "        depth=1\n",
    "        counts_df = pd.DataFrame(columns=['total','percentage'])\n",
    "        df_work = df.copy()\n",
    "        df_work[self.aggregator] = 0\n",
    "        if scramble:\n",
    "            df_work[self.target] = np.random.permutation(df_work[self.target].values)\n",
    "        for feature in self.feature_list:\n",
    "            value_df = df_work[[feature,self.target,self.aggregator]].groupby([feature,self.target]\n",
    "                                                                    ).agg(self.aggregator).reset_index()\n",
    "            if self.aggregator == 'count':\n",
    "                temp_rules_df['percentage'] = temp_rules_df['count']/temp_rules_df['total']\n",
    "                count_df = value_df[[feature,self.aggregator]].rename({self.aggregator:'total'},axis=1\n",
    "                                                                ).groupby([feature]).agg('sum').reset_index() \n",
    "                temp_rules_df = pd.merge(value_df,count_df,how='left',on=feature) \n",
    "            else:\n",
    "                temp_rules_df= value_df\n",
    "                \n",
    "            if self.target_value != 'Any':\n",
    "                condition = temp_rules_df[target] == self.target_value\n",
    "                temp_rules_df =temp_rules_df.loc[condition,:]\n",
    "            temp_rules_df = temp_rules_df.loc[temp_rules_df['total'] > self.size_threshold,:]\n",
    "            if (temp_rules_df[self.max_flag].max() > self.agg_threshold): \n",
    "                try_rule_list.append(feature)\n",
    "                counts_df = counts_df.append(temp_rules_df[counts_df.columns])\n",
    "                if (self.max_depth != 1):\n",
    "                    counts_df=counts_df.append(self.check_deep_layers(df_work,depth,[feature])\n",
    "                                               [counts_df.columns])\n",
    "        \n",
    "        counts_df=counts_df.reset_index()\n",
    "        \n",
    "        #display(counts_df)\n",
    "        \n",
    "        self.significance_plot(counts_df,scale)\n",
    "        \n",
    "        del try_rule_list\n",
    "        del value_df\n",
    "        del temp_rules_df\n",
    "        \n",
    "        return counts_df\n",
    " \n",
    "    def check_deep_layers(self,df_work,depth,high_rule):\n",
    "        depth = depth+1\n",
    "        try_rule_list = []\n",
    "        new_counts_df = pd.DataFrame(columns=['total','percentage'])\n",
    "        for feature in self.feature_list:\n",
    "            if feature in high_rule:\n",
    "                continue\n",
    "            value_df = df_work[high_rule + [feature,self.target,self.aggregator]].groupby(\n",
    "                               high_rule + [feature,self.target]).agg(self.aggregator).reset_index() \n",
    "            if self.aggregator == 'count':\n",
    "                count_df = value_df[high_rule + [feature,self.aggregator]].rename({self.aggregator:'total'},axis=1\n",
    "                                    ).groupby(high_rule + [feature]).agg('sum').reset_index()\n",
    "                temp_rules_df = pd.merge(value_df,count_df,how='left',on=high_rule+[feature])\n",
    "                temp_rules_df['percentage'] = temp_rules_df['count']/temp_rules_df['total']\n",
    "            else:\n",
    "                temp_rules_df= value_df\n",
    "            \n",
    "            if self.target_value != 'Any':\n",
    "                condition = temp_rules_df[target] == self.target_value\n",
    "                temp_rules_df = temp_rules_df.loc[condition,:]\n",
    "            \n",
    "            temp_rules_df = temp_rules_df.loc[temp_rules_df['total'] > self.size_threshold,:]\n",
    "            \n",
    "            if (temp_rules_df[self.max_flag].max() > self.agg_threshold):\n",
    "                var_list = high_rule +[feature]\n",
    "                new_counts_df = new_counts_df.append(temp_rules_df[new_counts_df.columns])\n",
    "                if (self.max_depth != depth):\n",
    "                    new_counts_df = new_counts_df.append(self.check_deep_layers(df_work,depth,var_list)\n",
    "                                                                                    [new_counts_df.columns])\n",
    " \n",
    "        depth = depth-1\n",
    "        \n",
    "        return new_counts_df \n",
    " \n",
    "        del try_rule_list\n",
    "        del new_counts_df\n",
    "        del more_counts_df\n",
    "        del temp_rules_df\n",
    "        del value_df\n",
    "        del count_df\n",
    "            \n",
    "    def significance_plot(self,counts_df,scale=10,scramble=False):\n",
    "        \n",
    "        if scale >1:\n",
    "            print(\"A scale of 10 is a 10x segmenter for fairly crude representations of the data field.\")\n",
    "            print(\"For finer representations a value of 5 or 2 is recommended.\")\n",
    "        else:\n",
    "            print(\"Warning: scale is too fine. Recommended values are 10,5, & 2. Quitting.\")\n",
    "            return\n",
    "              \n",
    "        # Create data\n",
    "        x = counts_df['percentage']\n",
    "        y = np.log10(counts_df['total'].astype('float'))\n",
    "\n",
    "        # Create heatmap\n",
    "        heatmap, xedges, yedges = np.histogram2d(x, y, bins=(10,10))\n",
    "        extent = [xedges[0], xedges[5], yedges[0], yedges[1]]\n",
    "\n",
    "        # Plot heatmap\n",
    "        plt.clf()\n",
    "        plt.title('Pythonspot.com heatmap example')\n",
    "        plt.ylabel('y')\n",
    "        plt.xlabel('x')\n",
    "        plt.imshow(heatmap, extent=extent)\n",
    "        plt.show()\n",
    "\n",
    "        # For use in tableau\n",
    "        counts_df['count']=0\n",
    "        counts_df[self.max_flag] = (10*scale*(counts_df[self.max_flag].round(2))).astype('int')\n",
    "        counts_df['total'] = (10**((scale*np.log10(counts_df['total'].astype('float')).round(2)/scale)\n",
    "                                  )).astype('int')\n",
    "        counts_df = counts_df[['percentage','total','count']].groupby(['total','percentage']).count() \\\n",
    "                                .unstack(fill_value=0).stack().reset_index()\n",
    "        display(counts_df)\n",
    "        \n",
    "        # save here for import into tableau heatmap\n",
    "        if scramble:\n",
    "            counts_df.to_csv(\"./rand_counts_scale\" + str(scale) +\".csv\")\n",
    "        else:\n",
    "            counts_df.to_csv(\"./real_counts_scale\" + str(scale) +\".csv\") \n",
    "                     \n",
    "    def save_model(self,filename=\"./model.pkl\"):\n",
    "        pickle.dump(self, open(filename, 'wb'))\n",
    "    \n",
    "    def load_model(self,filename=\"./model.pkl\"):\n",
    "        self = pickle.load(open(filename, 'rb'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting members  =  8124\n",
      "remaining members =  8092\n",
      "remaining members =  8068\n",
      "remaining members =  8044\n",
      "remaining members =  8020\n",
      "remaining members =  7970\n",
      "remaining members =  7906\n",
      "remaining members =  7762\n",
      "remaining members =  7686\n",
      "remaining members =  7674\n",
      "remaining members =  7650\n",
      "remaining members =  7626\n",
      "remaining members =  7602\n",
      "remaining members =  7578\n",
      "remaining members =  7434\n",
      "remaining members =  7402\n",
      "remaining members =  7326\n"
     ]
    }
   ],
   "source": [
    "# clf = RipOneRule(feature_list = ['sepal length','sepal width','petal length','petal width'],target='Class',\n",
    "#                 size_threshold = 8, agg_threshold = 0.8)\n",
    "clf = RipOneRule(feature_list = ['cap_shape','cap_surface','cap_color','bruises','odor','gill_attachment',\n",
    "                                 'gill_spacing','gill_size','gill_color','stalk_shape','stalk_root',\n",
    "                                 'stalk_surface_above_ring','stalk_surface_below_ring','stalk_color_above_ring',\n",
    "                                 'stalk_color_below_ring','veil_type','veil_color','ring_number','ring_type',\n",
    "                                 'spore_print_color','population','habitat'],target='type',\n",
    "                                  size_threshold = 8, agg_threshold = 0.8,max_depth=3)\n",
    "\n",
    "\n",
    "results = clf.fit(data_df)\n",
    "\n",
    "display(clf.predict(data_df))\n",
    "\n",
    "clf = RipOneRule(feature_list = ['cap_shape','cap_surface','cap_color','bruises','odor','gill_attachment',\n",
    "                                 'gill_spacing','gill_size','gill_color','stalk_shape','stalk_root',\n",
    "                                 'stalk_surface_above_ring','stalk_surface_below_ring','stalk_color_above_ring',\n",
    "                                 'stalk_color_below_ring','veil_type','veil_color','ring_number','ring_type',\n",
    "                                 'spore_print_color','population','habitat'],target='type',\n",
    "                                  size_threshold = 1, agg_threshold = 0.0,restart=True,max_depth=3)\n",
    "\n",
    "real_counts_df = clf.check_significance(data_df,scramble=False)\n",
    "random_counts_df = clf.check_significance(data_df,scramble=True)\n",
    "\n",
    "#print(results)\n",
    "#print(clf)\n",
    "\n",
    "# for mushroom set \n",
    "# y_mush = data['type']\n",
    "\n",
    "# x_mush = data.loc[:,'cap_shape':]\n",
    "\n",
    "# clf_mush = OneR()\n",
    "# results = clf_mush.fit(x_mush, y_mush)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
